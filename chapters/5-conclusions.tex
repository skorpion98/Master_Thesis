\chapter{Conclusions and Future Work} \label{chap_5}
\ \\

In this last chapter, we will summarize the work presented in this thesis. After that, we will present some directions for future work that could be explored to further improve our solution and the impact it can have in malware analysis research.

\section{Conclusion}
We first provided the concepts of fuzzing and sanitizers, a popular bug-detection approach in testing environments that has proven to be particularly effective, especially when combined with the ``Continuous-Fuzzing/Continuous-Integration'' pipeline that many modern organizations employ when developing their products. Then, we introduced autonomous fuzzing frameworks, how they work and described the chosen frameworks analyzed. Followed a description of the methodology used to analyze said frameworks along with how bugs were collected, analyzed and reported. Concludes a thorough analysis of the results, discussing how developers perceived the bug reports created and the implications of the behaviors observed with some case studies.

This study highlighted the popularity of autonomous fuzzing infrastructures and their effectiveness, but also that the lack of standardized approaches and design trade-offs are major contributors to their accuracy when it comes to automatically detecting and reporting bugs. Although the overall number of projects analyzed is not representative of the entire campaigns, the applied methodology still managed to discover several hundreds of previously unreported bugs, showing that the workflow adopted by the analyzed campaigns presents flaws that lead to overlooked bugs and potential vulnerabilities.

In today's world, where software development techniques are constantly changing and improving, it is crucial to ensure that all aspects of software development progress at the same pace, including the ``Testing'' phase: many organizations rely on automated testing, a time-efficient and effective solution, but that comes with its shortcomings as highlighted by this work. Therefore, while relying on autonomous testing techniques and infrastructures has proven its efficacy in time, developers should monitor, configure and employ these tools regularly and keep them up to date to ensure that also the testing environments can be as accurate and productive as possible.


\newpage
\section{Future Work}
This work highlighted relevant issues in the way fuzzing infrastructures detect bugs and vulnerabilities, providing both practical and research contributions. Still, we strongly believe this could be the starting point for more research in the field of autonomous fuzzing infrastructures. In particular, we foresee two main directions in which this thesis can be expanded.

We have shown how two of the most popular fuzzing infrastructures have issues in accurately detecting some bugs, leaving them accessible to malicious entities that can potentially use them to attack digital systems. While OSS-Fuzz and FuzzBench are two prominent examples of such infrastructures, more fuzzing infrastructures exist and are created as time passes: we believe expanding this analysis to more infrastructures would provide valuable information on how to design more accurate fuzzing infrastructures. Doing so would not only provide the practical value of finding overlooked bugs in specific environments, but could also provide new insights for other infrastructures to improve. For instance, the analysis of OSS-Fuzz showed the importance of sanitizers; FuzzBench, on the other hand, showed us the importance of always testing bugs in the latest version of the software as well as the ones used for testing. The relevance of these two pieces of information goes far beyond the boundaries of the aforementioned infrastructures, and we believe analyzing more of them will provide even more valuable information.

Other than testing more infrastructures, we believe this work by itself could be the starting point to create a set of guidelines on how to design accurate fuzzing infrastructures. We highlighted some critical points that are relevant to fuzzing in general, like the importance of sanitizers, the importance of testing the latest versions, and the danger of sharing too much information without proper checking. We believe all these insights could be aggregated to create a common framework to allow future development of fuzzing infrastructures to be as complete as possible, hopefully avoiding these mistakes in the future.
\chapter{Results} \label{chap_4}
\ \\
To correctly analyze and categorize the bugs discovered you need to perform \textit{bug deduplication}, a technique used to remove all inputs that produced the same results, which is crucial to remove unnecessary data from the results and make the bug analysis work less tedious for the developer.
\newline \newline
There are several strategies that can be applied to deduplicate bugs, but it's important to mention that there is no fool-proof methodology that produces perfect results on all possible scenarios, and this is because which information you are using as reference and their number will (almost always) lead to partial loss of data.
\newline
Given that this work focused primarily on OSS-Fuzz, I decided to employ the same approach used by them when generating bug reports, where they use the fields "Crash Type" and "Crash state" as the main references (Figure \ref{fig:issue}).
\newline
To do this, I used several Python scripts (one for each sanitizer and one for Valgrind) that analyzed the logs generated by the fuzzing sessions, and deduplication was performed by its \textit{"error type"} and the last 3 stack entries from where the error occurred.
\newline \newline
Then, \textit{bug triage} was performed, analyzing more in depth where the bug occurred, why it happened and assigning a priority in the range "Low", "Medium" and "High". This required a manual investigation of each bug individually, statically and dynamically (when available), to have a more clear understanding of the problem and potentially provide suggestions to the developers regarding the fix.
\newline
This step was also important to refine the previous deduplication step.
\newline
Assume the situation where two inputs triggered two (apparently) different bugs because they have distinct stack flows: if the bug originated from the same function, it meant that two different execution flows triggered the same error. In this case, a common practice is to fix one of them and then use the other input as an additional check for correctness.
\newline \newline
Finally, all recorded bugs for a project, along with their log and fuzz targets, were reported to their respective developers.
\newline \newline
At the moment of writing, unfortunately, not all developers answered and/or acknowledged the reported bugs.






\newpage
\section{OSS-Fuzz (aggiornare la tabella fino all'ultimo!)}
\ \\
\begin{adjustbox}{width=\textwidth,center}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\hline
\textbf{Project} & \textbf{Sanitizers} & \textbf{Queue size} & \textbf{Crashes} & \textbf{ASan} & \textbf{MSan} & \textbf{UBSan} & \textbf{Unique} & \textbf{Confirmed}  \\ 
\hline
binutils         & ALL                 & $20,274$            & $0$              & $0$           & $1$           & $0$            & $1$             & $1$                 \\
harfbuzz         & ALL                 & $23,357$            & $0$              & $0$           & $1$           & $0$            & $1$             & $1$                 \\
imagemagick      & ALL                 & $9,470$             & $0$              & $0$           & $6$           & $1$            & $7$             & $1$                 \\
libxml2          & ALL                 & $13,474$            & $0$              & $0$           & $0$           & $0$            & $0$             & $0$                 \\
skia             & ALL                 & $18,295$            & $0$              & $0$           & $0$           & $0$            & $0$             & $0$                 \\ 
\hline
ghostscript      & A+M                 & $9,917$             & $0$              & $0$           & $32$          & $1$            & $33$            & $33$                \\
libyang          & A+M                 & $8,745$             & $0$              & $0$           & $0$           & $0$            & $0$             & $0$                 \\
myanmar-tools    & A+M                 & $448$               & $0$              & $0$           & $0$           & $0$            & $0$             & $0$                 \\
openjpeg         & A+M                 & $8,856$             & $0$              & $0$           & $0$           & $0$            & $0$             & $0$                 \\
wasmedge         & A+M                 & $9,454$             & $0$              & $0$           & $0$           & $0$            & $0$             & $0$                 \\ 
\hline
cairo            & A+UB                & $15,870$            & $0$              & $1$           & $1$           & $28$           & $30$            & $0$                 \\
clamav           & A+UB                & $6,742$             & $0$              & $0$           & $0$           & $2$            & $2$             & $0$                 \\
freerdp          & A+UB                & $7,607$             & $0$              & $0$           & $0$           & $1$            & $1$             & $1$                 \\
tarantool        & A+UB                & $10,987$            & $0$              & $0$           & $1$           & $0$            & $1$             & $1$                 \\
vlc              & A+UB                & $16,018$            & $2$              & $1$           & $2$           & $4$            & $9$             & $5$                 \\ 
\hline
fwupd            & ASan only                & $5,843$             & $0$              & $0$           & $0$           & $0$            & $0$             & $0$                 \\
glslang          & ASan only                & $14,534$            & $0$              & $1$           & $1$           & $0$            & $1$             & $1$                 \\
inchi            & ASan only                & $12,034$            & $0$              & $1$           & $4$           & $3$            & $8$             & $8$                 \\
radare2          & ASan only                & $9,914$             & $0$              & $1$           & $0$           & $9$            & $10$            & $10$                \\
zeek             & ASan only                & $8,390$             & $0$              & $0$           & $1$           & $5$            & $6$             & $6$                 \\ 
\hline
fluent-bit       & NONE                & $4,968$             & $0$              & $0$           & $1$           & $1$            & $2$             & $2$                 \\
gpac             & NONE                & $22,917$            & $2$         & $0$           & $25$          & $0$            & $27$            & $27$                \\
libdwarf         & NONE                & $7,667$             & $0$              & $0$           & $0$           & $0$            & $0$             & $0$                 \\
libredwg         & NONE                & $46,160$            & $0$              & $1$           & $3$           & $0$            & $4$             & $4$                 \\
serenity         & NONE                & $9,940$             & $0$              & $0$           & $1$           & $1$            & $2$             & $2$                 \\
\hline
\end{tabular}
\end{adjustbox}{}
\ \\ \newline
The above table shows the aggregated results for OSS-Fuzz.
\newline \newline


\newpage
\matteo{Put here all the data, make a table with aggregated results.}

\matteo{Say something about the presence of sanitizers and how many bugs we found when that sanitizer was present vs when it was not present}

\matteo{Say something about why those bugs have been overlooked. I think we have a past mail on this; our hypothesis is that, when a sanitizer was not enabled, then trivially the fuzzer could not detect certain bugs. When that sanitizer was enabled maybe certain campaigns with one sanitizer generated a testcase that triggered a bug with another sanitizer and was also saved in the queue (e.g. it produced new coverage).}
\ziosaba{Tornando ora al problema principale, ovvero perché noi abbiamo trovato bug che non sono stati riportati automaticamente dal sistema, ho alcune idee. Un primo motivo banale, come da voi citato nella mail precedente, è la mancanza di uno specifico sanitizer nei test effettuati dal sistema: se lo sviluppatore non ha richiesto a OSS-Fuzz di testare il proprio codice con MSan perché non ha fornito binari instrumentati con tale sanitizer, è ovvio che non verranno mai trovati bug di MSan. Un secondo motivo che mi viene in mente è appunto l'utilizzo di fuzzer diversi: rianalizzando i file "project.yaml" dei 25 progetti finali (trovate le statistiche nel solito foglio Excel), poco meno della metà di questi hanno implementato AFL nei test automatizzati e questo mi ha portato ad effettuare due osservazioni.
La prima è relativa ai progetti con AFL, che nella statistica generale hanno presentato in media un minor numero di bug nelle rispettive categorie (ad eccezione di Cairo che sembra ignorare completamente problemi relativi ad overflow e rappresentazione dei dati nei rispettivi tipi), il che ha perfettamente senso poiché ci pensa già il sistema a testarli. La seconda è invece relativa a tutti quei progetti che sono implementati con uno (o più) fuzzer ma in cui AFL non è incluso: tornando al discorso di prima, testare un programma con un fuzzer diverso da quelli già usati ma usando un corpus costruito dagli input generati da altri fuzzer che si basano su tecniche diverse, può portare ad ottenere nuovi risultati mai visti in precedenza. Una terza spiegazione riguarda invece il continuo cambiamento delle code. Come accennato sopra, tra una sessione di fuzzing e l'altra ci sono diversi algoritmi che vanno a modificarne il contenuto, e nello specifico voglio soffermarmi sui "MAB algorithms" che sono stati programmati per massimizzare il code coverage.
Supponiamo di partire da una coda in cui è presente un testcase che non genera un bug per la build corrente: se al termine della sessione di fuzzing tale testcase viene rimosso dalla coda, in una build successiva questo potrebbe invece portare ad un bug che però non sarà mai riportato se la coda utilizzata nella successiva sessione non contiene un altro input che porta allo stesso risultato. Pertanto, utilizzare una coda vecchia di diversi mesi su un progetto aggiornato potrebbe portare a risultati inaspettati. Ultimo motivo a cui avevo pensato è invece relativo alle scelte progettuali di ClusterFuzz. Leggendo la documentazione, questa pagina ha catturato la mia attenzione, di cui riporto di seguito un estratto: ClusterFuzz does not consider testcases that do not reliably reproduce as important. However, if a crash state is seen very frequently despite not having a single reliable testcase for it, ClusterFuzz will file a bug for it. When ClusterFuzz finds a reliably reproducible testcase for the same crash state, it creates a new report and deletes the older report with the unreliable testcase. Durante la scrittura dei report, ci sono state diverse occasioni in cui i bug trovati non erano consistenti, ovvero appunto crash o sanitizer-errors che non si presentavano ad ogni esecuzione del codice.
Sapendo quanto citato sopra, non mi meraviglia che tali bug non siano stati catturati e reportati dal sistema: infatti, in tali report abbiamo tenuto a precisare che il bug non si presentava ad ogni esecuzione del codice eo che produceva risultati diversi a seconda che il binario fosse compilato con o senza}

\matteo{Highlight the crashes we found in gpac and in vlc; also say that we found gpac ones while they were still in private disclosure}

\matteo{Do one or two case studies (in subsetions); I would talk about gpac, we have something to say about all the MSan bugs we found and the interaction with the CEO. The developer also wrote in their report on github that MSan bugs were giving them non-deterministic behaviors in their software. Then maybe vlc? It has 2 sanitizer and yet we found bugs for all sanitizers plus two crashes.}

\matteo{Make a new subsection and talk about what can we learn about this, and how can this issue be fixed by Google.}




 \newpage
\section{FuzzBench (aggiornare la tabella fino all'ultimo!)}

\matteo{Kind of the same we did for OSS-Fuzz, let's wait for some data before drafting something, I guess.}

\matteo{We can distinguish between "regular" fuzzbench and sbft? Let's see what the numbres say.}


\newpage
\section{Developers' responses}
Discuss the reports and the developers' answers, including some considerations about their responses...


\newpage
\section{Discussion}
Discuss the overall results, what was expected and what was unexpected...
\newline \newline
Discuss the importance of such results, what can be inferred...
\newline \newline
Talk about what could/should be done to improve the situation...

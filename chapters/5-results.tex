\chapter{Results} \label{chap_4}
\ \\
To correctly analyze and categorize the bugs discovered you need to perform \textit{bug deduplication}, a technique used to remove all inputs that produced the same results, which is crucial to remove unnecessary data from the results and make the bug analysis work less tedious for the developer.
\newline \newline
There are several strategies that can be applied to deduplicate bugs, but it's important to mention that there is no fool-proof methodology that produces perfect results on all possible scenarios, and this is because which information you are using as reference and their number will (almost always) lead to partial loss of data.
\newline
Given that this work focused primarily on OSS-Fuzz, I decided to employ the same approach used by them when generating bug reports, where they use the fields "Crash Type" and "Crash state" as the main references (Figure \ref{fig:issue}).
\newline
To do this, I used several Python scripts (one for each sanitizer and one for Valgrind) that analyzed the logs generated by the fuzzing sessions, and deduplication was performed by its \textit{"error type"} and the last 3 stack entries from where the error occurred.
\newline \newline
Then, \textit{bug triage} was performed, analyzing more in depth where the bug occurred, why it happened and assigning a priority in the range "Low", "Medium" and "High". This required a manual investigation of each bug individually, statically and dynamically (when available), to have a more clear understanding of the problem and potentially provide suggestions to the developers regarding the fix.
\newline
This step was also important to refine the previous deduplication step.
\newline
Assume the situation where two inputs triggered two (apparently) different bugs because they have distinct stack flows: if the bug originated from the same function, it meant that two different execution flows triggered the same error. In this case, a common practice is to fix one of them and then use the other input as an additional check for correctness.
\newline \newline
Finally, all recorded bugs for a project, along with their log and fuzz targets, were reported to their respective developers.
\newline \newline
At the moment of writing, unfortunately, not all developers answered and/or acknowledged the reported bugs.






\newpage
\section{OSS-Fuzz (aggiornare la tabella fino all'ultimo!)}
\ \\
\begin{adjustbox}{width=\textwidth,center}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\hline
\textbf{Project} & \textbf{Sanitizers} & \textbf{Queue size} & \textbf{Crashes} & \textbf{ASan} & \textbf{Valgrind} & \textbf{UBSan} & \textbf{Total} & \textbf{Confirmed}  \\ 
\hline
binutils         & ALL                 & $20,274$            & $0$              & $0$           & $1$           & $0$            & $1$             & $1$                 \\
harfbuzz         & ALL                 & $23,357$            & $0$              & $0$           & $1$           & $0$            & $1$             & $1$                 \\
imagemagick      & ALL                 & $9,470$             & $0$              & $0$           & $6$           & $1$            & $7$             & $1$                 \\
libxml2          & ALL                 & $13,474$            & $0$              & $0$           & $0$           & $0$            & $0$             & $0$                 \\
skia             & ALL                 & $18,295$            & $0$              & $0$           & $0$           & $0$            & $0$             & $0$                 \\ 
\hline
ghostscript      & A+M                 & $9,917$             & $0$              & $0$           & $32$          & $1$            & $33$            & $33$                \\
libyang          & A+M                 & $8,745$             & $0$              & $0$           & $0$           & $0$            & $0$             & $0$                 \\
myanmar-tools    & A+M                 & $448$               & $0$              & $0$           & $0$           & $0$            & $0$             & $0$                 \\
openjpeg         & A+M                 & $8,856$             & $0$              & $0$           & $0$           & $0$            & $0$             & $0$                 \\
wasmedge         & A+M                 & $9,454$             & $0$              & $0$           & $0$           & $0$            & $0$             & $0$                 \\ 
\hline
cairo            & A+UB                & $15,870$            & $0$              & $1$           & $1$           & $28$           & $30$            & $0$                 \\
clamav           & A+UB                & $6,742$             & $0$              & $0$           & $0$           & $2$            & $2$             & $0$                 \\
freerdp          & A+UB                & $7,607$             & $0$              & $0$           & $0$           & $1$            & $1$             & $1$                 \\
tarantool        & A+UB                & $10,987$            & $0$              & $0$           & $1$           & $0$            & $1$             & $1$                 \\
vlc              & A+UB                & $16,018$            & $2$              & $1$           & $2$           & $4$            & $9$             & $5$                 \\ 
\hline
fwupd            & ASan only                & $5,843$             & $0$              & $0$           & $0$           & $0$            & $0$             & $0$                 \\
glslang          & ASan only                & $14,534$            & $0$              & $1$           & $1$           & $0$            & $1$             & $1$                 \\
inchi            & ASan only                & $12,034$            & $0$              & $1$           & $4$           & $3$            & $8$             & $8$                 \\
radare2          & ASan only                & $9,914$             & $0$              & $1$           & $0$           & $9$            & $10$            & $10$                \\
zeek             & ASan only                & $8,390$             & $0$              & $0$           & $1$           & $5$            & $6$             & $6$                 \\ 
\hline
fluent-bit       & NONE                & $4,968$             & $0$              & $0$           & $1$           & $1$            & $2$             & $2$                 \\
gpac             & NONE                & $22,917$            & $2$         & $0$           & $25$          & $0$            & $27$            & $27$                \\
libdwarf         & NONE                & $7,667$             & $0$              & $0$           & $0$           & $0$            & $0$             & $0$                 \\
libredwg         & NONE                & $46,160$            & $0$              & $1$           & $3$           & $0$            & $4$             & $4$                 \\
serenity         & NONE                & $9,940$             & $0$              & $0$           & $1$           & $1$            & $2$             & $2$                 \\
\hline
TOTAL BUGS   &   &   &$4$   &$6$   &$80$   &$56$   &$145$   &$103$       \\
\hline
\end{tabular}
\end{adjustbox}{}
\ \\ \newline
The above table shows the aggregated results for OSS-Fuzz.
\newline \newline
First and foremost, developers have different approaches to bug reporting. 
\newline
The OSS-Fuzz reports were submitted during September, and there are still just under 1/3 of the discovered bugs that have yet to be acknowledged by their respective developers. Although most of them were related to use-of-uninitialized-memory and undefined behaviors, they would require little to none effort to solve them, which shows how many open-source developers either do not have enough time and resources to fix them, or they simply do not care about reports on trivial bugs that are not security relevant and postpone them (sometimes indefinitely).
\newline \newline
Then, we can see the effects of using sanitizers during tests.
\newline
While the projects that were being tested using all sanitizers produced only a total of 9 bugs, the remaining 136 were evenly distributed across the remaining categories: this is a good example of how a sanitizer is beneficial to your program but is not a replacement for programs written following good coding practices.
\newline
"Ghostscript" uses MSan, yet it has the highest number of memory-related bugs.
\newline
"Cairo" uses UBSan, yet it has the highest number of undefined-behavior bugs.
\newline \newline
We can also infer the popularity of each sanitizer.
\newline
ASan is the most popular one, reflected on the number of bugs it discovered.
\newline
UBSan is also among the most used ones, although the statistic shown here is biased by "Cairo": while this project is affected by many UB bugs, specifically improper conversion of data between different types, almost all previous reports found on the issue tracker related to these problems were simply marked with "Wont Fix".
\newline
Finally, Valgrind's memory analysis yielded the highest number, highlighting how MSan was often not used due to its reports containing too many false positives.


\newpage
The reasons why this work discovered so many bugs are up for debate, as the inner workings of such infrastructures are not publicly available most likely due to security reasons, but there are some key hypothesis that can be mentioned.
\newline \newline
A trivial explanation lies in the developers' choices when it comes to selecting which sanitizers and fuzzing engines will be used for the tests.
\newline
Regarding the sanitizers, one would expect to find many bugs of a certain type when introducing a sanitizer that is not already being used by OSS-Fuzz. Moreover, each sanitizer generates different types of testcases that may produce new coverage when testing the program with another sanitizer.
\newline
Also fuzzing engines, similarly to sanitizers, use different approaches and strategies when testing a program: given that each fuzzer generates different new testcases and produces different coverage, using the same corpus with different fuzzers may produce substantially different results.
\newline
Moreover, although not shown in the previous table, less than half of the projects tested included AFL as one of its fuzzing engines used for the tests, while the others relied on the default option (libfuzzer) or a set of fuzzers where AFL was not included: it's interesting to mention that all the projects using AFL yielded (on average) less bugs that the others, which again highlights why it is considered the current state-of-the-art.
\newline \newline
Another reasoning may lies in the algorithms employed by ClusterFuzz regarding the management of fuzzing queues.
\newline
Each time a project undergoes a fuzzing session, a new set will be produced containing all the interesting input discovered during this test along with any new testcases generated by the fuzzer used, and all these inputs will later from the corpus used as input for future fuzzing sessions: to make sure that the size of this corpus does not explode over time, you need \textit{bug deduplication} and \textit{pruning} techniques.
\newline
We already established how ClusterFuzz performs \textit{deduplication}, specifically "error type" and the last 3 stack entries, and also mentioned that there is no general solution to this process that does not involve some degree of information loss.
\newline
The same can be said about \textit{pruning}, which is the process of removing all unnecessary and non-relevant inputs from a corpus. ClusterFuzz employ several \textit{"Multi-Armed Bandit (MAB)"} \cite{mab} algorithms, whose statistical explanation will be omitted for the sake of simplicity, in the following way: at the end of each fuzzing session, ClusterFuzz must decide whether to prioritize inputs that caused bugs or inputs that produced new coverage, using regress knowledge and implementation choices as reference. Given that ClusterFuzz's developers prioritize code coverage, most inputs from a fuzzing queue will contain that type of inputs, implying that pruning will most likely remove some inputs causing bugs from future fuzzing queues.
\newline \newline
Related to the problem of pruning, there is also the versioning problem.
\newline
Let's assume that it exists a specific input on the current version of the program that causes a bug. Then, the developers release a new version that introduces some changes, along with a fix for the problem, and the input is automatically removed by ClusterFuzz after a successful test as it is not relevant anymore: just because that particular input has been fixed and removed from the queue, it does not necessarily mean that particular type of error has been permanently fixed unless another input in future fuzzing queues reproduces the same problem. This is the reason why keeping old inputs that caused bugs and testing old corpora on new versions is useful, as it is not uncommon for developers to reintroduce old bugs during changes, and they might produce interesting and unexpected results.


\newpage
Finally, we mention how ClusterFuzz handles bugs that cannot be reliably reproduced \cite{unreliable}: 
\newline \newline
\textit{"ClusterFuzz does not consider testcases that do not reliably reproduce as important. However, if a crash state is seen very frequently despite not having a single reliable testcase for it, ClusterFuzz will file a bug for it. When ClusterFuzz finds a reliably reproducible testcase for the same crash state, it creates a new report and deletes the older report with the unreliable testcase."}
\newline \newline
During this analysis, there were few inputs that did not produce a bug consistently, something that was obviously mentioned in the reports containing them. Given that it is unknown how many times a crash must be "seen" by ClusterFuzz before a generic bug report for it is produced, it's possible that the reports generated during this work may be related to bugs already known by ClusterFuzz but that were not appearing enough times to be considered relevant.


\subsection{Case study: GPAC}
\subsection{Case study: VLC}


\newpage
\matteo{Highlight the crashes we found in gpac and in vlc; also say that we found gpac ones while they were still in private disclosure}
\ziosaba{I think that introducing this topic when talking about the projects in their respective subsection is better? Don't know exactly how to tackle them here in few meaningful lines}

\matteo{Do one or two case studies (in subsetions); I would talk about gpac, we have something to say about all the MSan bugs we found and the interaction with the CEO. The developer also wrote in their report on github that MSan bugs were giving them non-deterministic behaviors in their software. Then maybe vlc? It has 2 sanitizer and yet we found bugs for all sanitizers plus two crashes.}

\matteo{Make a new subsection and talk about what can we learn about this, and how can this issue be fixed by Google.}
\ziosaba{Why not moving this topic in the "Discussion" section?}






 \newpage
\section{FuzzBench (aggiornare la tabella fino all'ultimo!)}

\matteo{Kind of the same we did for OSS-Fuzz, let's wait for some data before drafting something, I guess.}

\matteo{We can distinguish between "regular" fuzzbench and sbft? Let's see what the numbres say.}

\subsection{Case study: ???}

\newpage
\section{Developers' responses}
Discuss the reports and the developers' answers, including some considerations about their responses...


\newpage
\section{Discussion}
Discuss the overall results, what was expected and what was unexpected...
\newline \newline
Discuss the importance of such results, what can be inferred...
\newline \newline
Talk about what could/should be done to improve the situation...

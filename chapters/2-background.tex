\chapter{Background} \label{chap_2}

\matteo{Use this space to introduce the chapter. "In this chapter we will provide the necessary background for this thesis. We will start by introducing how xxx works, with an in-depth analysis of its yyy component and its effects on zzz. Then, we will discuss www [...]. Finally, we will conclude by detailing the inner working of vvv [...]}
\ziosaba{Do I have to repeat this "chapter introduction" for all chapters? Also, what else there is to say here?} \newline

This chapter will provide all necessary background information regarding the tools employed in this thesis, their functionality, and the objects of analysis.
\newline \newline
We will start by introducing the concept of \textit{fuzz testing}, which has been extensively used in this work, and the results it achieved over the years. 
\newline
Follows an in-depth analysis of the different approaches to this technique, each one with its pros and cons, and a brief explanation of how a fuzz session works and the results it produces.
\newline \newline
Then, we define what is a \textit{fuzz tester} (or simply \textit{fuzzer}), along with a description of its main components and the two main categories of fuzzers.
\newline \newline
We later introduce the \textit{sanitizers}, tools that were crucial to improve the effectiveness of automated testing, providing a brief analysis over the different types of sanitizers available and showcasing the ones used in this work along with their pros and cons.  
\newline \newline
We also provide a short insight about the definition of "Open-Source Software", what it is and why it became so popular and important in the modern era, highlighting some of the most commons problems faced by open-source developers.
\newline \newline
Finally, we show the automated testing infrastructures analyzed in this thesis, detailing their inner working and shortcomings that will be highlighted by this work.
\newline \newline
\ziosaba{What else??? Maybe try to make it a little longer?}





\newpage
\section{Fuzz Testing}
In the context of software development, \textit{fuzz testing} (or \textit{fuzzing}) refers to an automated software technique which focuses on providing random data to a program, with the objective of creating invalid and unexpected inputs that may potentially trigger crashes, assertions or memory leaks.
\newline 
This allows developers to test their programs for so called "corner-cases", meaning situations hard or complex to reproduce when the program is being properly used but, but that could still lead to unexpected and/or potential malicious behavior and thus should be properly handled. 
\newline \newline
Fuzzers can be categorized using the 3 following characteristics:
\begin{itemize}
    \item how new inputs are generated (\textit{generation-based} or \textit{mutation-based})
    \item whether they are aware of the input structure (\textit{smart fuzzers}) or not (\textit{dumb fuzzers})
    \item whether they are aware of the program structure (\textit{white-box}), partially aware (\textit{gray-box}) or not (\textit{black-box})
\end{itemize}
\ \\
It's important to mention that there is a trade-off between results obtained, time spent testing and resources available.
\newline
Fuzzing is a resource-intensive process, putting the CPU cores under heavy load while also generating potentially enormous log files with tests run for several hours or even days, but preparing the right set of inputs that will be used to perform the tests is crucial to obtain good results.
\newline
Moreover, during a fuzzing session, there will be a point in time where either the inputs generated by the fuzzer will not trigger new execution flows or no new bugs are discovered for quite some time: reaching this state does not mean that the program is finally free of other bugs, as the conditions to trigger them might simply have not happened yet, and it's obviously impossible to know when this will happen as bugs may be discovered randomly.
\newline \newline \newline
Among the many results achieved thanks to this technique, two honorable mentions can be linked to the popular fuzzer \textit{American Fuzzy Lop} (also known as \textit{AFL} \cite{AFL}).
\newline
In September 2014, AFL discovered "Shellshock" \cite{shellshock} (also known as "Bashdoor"), a family of security bugs affecting the Unix Bash shell that allowed malicious users to execute arbitrary commands without confirmation.
\newline
In April 2015, AFL discovered the famous "Heartbleed" \cite{heartbleed} bug in OpenSSL, which allowed malicious users to decipher encrypted communications from the TLS protocol. 
\newline \newline
This thesis will focus on two campaigns maintained by Google called \textit{OSS-Fuzz} \cite{ossfuzz_docs} and \textit{FuzzBench} \cite{{fuzzbench_docs}}: the first is a free platform that allows open-source developers to fuzz their programs autonomously relying on the computing resources provided by the Google Cloud service, while the latter allows fuzzer developers to test and improve their tools on real-word benchmarks thanks to automated tests and reports.
More specifically, it revolves around fuzzing some of the projects that have been implemented in these repositories using alternative approaches, trying to discover new and relevant bugs that will be then securely reported and disclosed to their developers in hope to have them fixed.

\newpage
We mainly distinguish between 3 types of fuzz testing:
\begin{itemize}
    \item \textbf{application fuzzing:} used for UI elements (such as buttons, input fields) or command-line programs, tests may include high-frequency inputs, providing random/invalid content and inputs exceeding the expected size
    \item \textbf{protocol fuzzing:} used to test the behavior of network elements and servers when invalid messages are sent over a chosen protocol, useful to ensure that such content is not misinterpreted and potentially executed as commands
    \item  \textbf{file format fuzzing:} used for programs that accept "structured inputs", meaning files that have a precise and standard format (like .doc, .jpg) which the fuzzer can modify with the intention of triggering unwanted behavior
\end{itemize}
This work will focus on testing programs that accept input files to provide their services, and for this reason several corpora were used to instruct the fuzzer on the appropriate file format. This will be discussed more in depth in section \ref{fuzzers}.
\newline \newline \newline
One of the main requirements in fuzz testing is to achieve a high degree of \textbf{code coverage}: this statistic measures the percentage of source code executed in a test session, therefore achieving a high value means lowering the chance of leaving parts of the program with undetected bugs. Given this, one could argue that the fuzzer should be aware of the program's structure to maximize the coverage, but given that this process requires a non-trivial overhead it comes with its own trade-offs.
\newline \newline
For this reason, we define 3 different approaches.
\newline \newline
The \textit{black-box testing} involves using a fuzzer that is completely unaware of the program's structure, therefore assumes the program as a simple machine that takes a random input and generates an according output. This approach is relatively fast, can be easily parallelized and has good scalability. On the other hand, it will most likely find only "surface" bugs, i.e. they do not require particular conditions to be met to be triggered.     
\newline \newline
The \textit{white-box testing} involves using a fuzzer that employs "program analysis" techniques to systematically explore and reach critical program locations through meticulously crafted inputs, allowing you to discover bugs that could be potentially hidden deep in the program, although heavily relying on this knowledge implies that bugs related to unknown aspects of the program can be easily missed. While this approach is arguably the most effective one, the time used to analyze the program as well as generating such specialized input exponentially increases with the program complexity.
\newline \newline
The \textit{gray-box testing} attempts to integrate the best aspects of both approaches: use a minimal amount of knowledge over the program's structure to achieve a sufficient degree of code coverage such that the results obtained are satisfactory. This is usually done thanks to "instrumentation", discussed in section \ref{sanitizers}.
The previously mentioned AFL, which has been extensively used in this work, falls in this category.

\newpage
A fuzzing session usually yields two outputs: a set of "interesting" inputs and a list of potential bugs.
\newline \newline
Whenever a generated input results in a new execution flow, it is saved along with others "interesting inputs", which can be used in subsequent fuzzing sessions to provide the fuzzer with even more information about the structure of a good input that explores the deepest parts of the program. Then, the developer might decide to analyze and group these lists, to create a new one that does not contain duplicates and/or inputs triggering the same execution flows, which is particularly useful to ensure that the size does not explode over time.
\newline \newline
Regarding the discovering of bugs, a fuzzer has to be sensible enough to distinguish between crashing and non-crashing inputs without having full knowledge over the program tested, and "sanitizers" are used to inject assertions that make the program crash when a particular kind of failure is detected. Section \ref{sanitizers} explains this concept more thoroughly.
\newline
Then, the fuzzer might produce one (or more) inputs that triggered different kind of bugs, and the developer has to perform what is called \textit{bug triage}:
\begin{itemize}
    \item execute each input individually and observe the output
    \item determine which kind of error occurred and why, maybe also introducing debugging tools
    \item fix the bug entirely, if possible, or at least patch the problem as much as possible
    \item ensure that the bug does not occur in future fuzzing sessions by including the triggering input(s) in the set used for the tests
\end{itemize}
\ \\
Finally, it's important to recall that running a fuzzing session for several days or weeks without bugs does not necessarily imply that the program is secure: this process is driven by randomness, initial inputs provided and environment used, meaning also that each fuzzing session will always result in slightly different results and coverage achieved. 
\newline
Yet, testing a program on all possible inputs is obviously impossible.
\newline
Moreover, it easily generates false positives (trivial or benign warnings) and false negative results (incorrect or misleading results), further overloading the already tedious work of manually assessing each bug.

\ \\ \newline \newline
\ziosaba{Explain other shortcomings of fuzz testing, maybe also available solution to mitigate them??? Not really sure what else to say}



\newpage
\section{Different types of Fuzz Testers} \label{fuzzers}
\ziosaba{I'm not entirely sure that this section should be merged with the previous one, mainly because they talk about two distinct topics, but if you think that it's better to merge them I will fix it later}

A fuzzer is composed by the following key elements. \cite{afl_docs}
\newline \newline
\textbf{Observer.}\ \ \ Provides information observed during the execution of a program to the fuzzer. Such information may be relatively simple, like the total running time for a single test and its output, to more advanced ones, like the maximum depth of the stack in a single execution. They are usually not preserved across many execution, unless an "interesting input" is encountered, in which case they are relayed to other nodes to improve future fuzzing sessions. 
\newline \newline
\textbf{Executor.}\ \ \ Responsible for defining how the program will be executed and the arguments passed on each run. The input for a single test is provided either by writing it in some specific memory location or passed as argument to a so called "harness function", although each fuzzer has its own implementation of this element. Given this, we briefly mention few standard functionalities that compose this element. The \textit{InProcessExecutor} runs the "harness function" and provides crash detection. The \textit{ForkServerExecutor} is responsible for spawning different child processes to fuzz. The \textit{TimeoutExecutor} wraps and installs a timeout for another running executor.
\newline \newline
\textbf{Feedback.}\ \ \ Classifies the result of a single execution and determines if the initial input is "interesting" or not, which is usually determined by retrieving information for the observers and analyzing the updated coverage map. It's also possible to define several of these elements, each one with its own objective (crashes, timeout, new execution flow discovered), and combine them in boolean expression to collect more fine-grained results.
\newline \newline
\textbf{Input.}\ \ \ Data taken from an external source and provided to the tested program to observe its behavior, usually in the form of bytes arrays. The first fuzzing session takes a set of inputs that is defined and provided by the developer itself, while future fuzzing session will also rely on previously discovered "interesting inputs". 
\newline \newline
\textbf{Testcase.}\ \ \ Defined as an input and a set of related metadata, like ID, description and expected results.
\newline \newline
\textbf{Corpus.}\ \ \ Location where testcases are stored, usually disk or memory. An example of input corpus may be composed by several testcases with the same properties, like crashing the program under a specific situation. An example of output corpus may be composed by all the testcases that are considered "interesting".
\newline \newline \newline
This thesis focused on using fuzzers capable of generating new structured inputs for the tested program starting from an initial \textit{seed}, specifically files with a precise format, allowing them to distinguish between a valid and an invalid example. 
\newline
However, even files that do not necessarily follow the defined structure can still be used as input for fuzzing, maybe with an even higher chance of triggering unwanted or unexpected behavior: for this reason, an effective fuzzer should be capable of generating inputs that are "valid enough" so that they are not rejected from the program's parser, and "invalid enough" to potentially trigger corner cases.


\newpage
We finally define the two main types of fuzzers.
\newline \newline
The \textbf{mutational-based fuzzers} require a corpus of seed inputs as reference, and generate new inputs by applying "mutators" on the provided seeds: these operations range from flipping single bits or bytes on a given input or performing mathematical operations over them, to adding or removing said information, sometimes even completely randomizing its content. Furthermore, these mutation may be mixed-and-matched into long sequences, to generate even more new inputs.
\newline
A \textit{smart mutational fuzzer} might leverage its knowledge on the input model to switch between different types of inputs, although this information is not always available.
\newline
A \textit{dumb mutational fuzzer}, like AFL, employs random mutations using as reference the content of "interesting inputs", which usually results in a much lower proportion of valid inputs generated.
\newline \newline
The \textbf{generation-based fuzzers} generate new inputs from scratch, usually relying on a good source of randomness to perform this operation, and for this reason they do not depend on the existence of a good corpus nor its quality.
\newline
A \textit{smart generation fuzzer} might leverage the input model provided by the developer to generate valid new inputs, although this information may not always be available.
\newline
A \textit{dumb generation fuzzer} attempts to generate new inputs without any reference, oftentimes putting more stress on the program's parser rather than the program itself, as they generate an overwhelmingly amount of invalid inputs.


\newpage
\section{Sanitizers} \label{sanitizers}
A \textit{code sanitizer} is a tool used to detect bugs in a program during both compilation and runtime, and this is done by "instrumenting" the code.
\newline
The act of "instrumentation" refers to modifying either the source code or binary code that is being compiled to add some additional functionalities and references that can be used by other tools to perform code analysis, logging or profiling.
\newline
It's important to notice that this process is not only severely limited by the single execution flow analyzed, but also introduces a non-trivial overhead both in terms of increased execution time and memory usage, which may cause problems when it comes to managing computing resources.
\newline \newline
Although code sanitizers should be a standard practice in the development cycle of a program, unfortunately this is not the case as introducing these tools not only requires extensive tests to check for potential errors, but also because they do not always interact very well with shared libraries or other external dependencies.
\newline
It is also extremely important to mention that these bug detection tools are not meant to be linked against production executables, as their runtime was not developed with security-sensitive constraints and may compromise the security of the released product. \cite{asan_docs}\cite{msan_docs}\cite{ubsan_docs}
\newline \newline
We mainly distinguish between \textit{compile} and \textit{binary} sanitizers.
\newline
A \textit{compile sanitizer} is a tool that performs instrumentation at compile-time, meaning that it introduces functions and libraries that will be then added to the original source code, effectively altering its default behavior. This allows them not only to analyze the program during its execution, but also to provide warnings and error during the compilation process as they are capable of achieving full coverage given that the analysis is performed on the original code and includes all possible paths. Moreover, given that the compiled binary is immateriality ready to be executed, the added overhead is usually low resulting in fast execution times.
\newline
A \textit{binary sanitizer} is instead a dynamic binary instrumentation (DBI) framework that performs "Just-in-Time" (JIT) compilation to introduce additional instructions during the execution of a program, without altering its original source code. They often rely on dynamic binary analysis (DBA) tools to analyze programs at run-time at the level of the machine code. Its main drawback is that the analysis is dependent on the input given and the resulting execution flow, therefore they are not capable of achieving full code coverage. Finally, this analysis involves attaching another external process to instrument the original one, causing massive slowdowns.  
\newline \newline
This work focused on using several compile sanitizers developed by Google \cite{san_repo} as part of its tools provided to open-source developers to help them in improving their programs and making them more secure, along with the Valgrind \cite{valgrind_web} binary sanitizer. These tools proved to be particularly effective when combined with fuzzing, due to its ability to trigger bugs.


\newpage
\subsection{ASan and LSan}
The \textit{Address Sanitizer} \cite{serebryany2012addresssanitizer} is a memory error detection tool for C/C++ that helps developers to find and fix any out-of-bounds accesses to heap, stack and global objects as well as use-after-free bugs, and its one of the most widely used and effective sanitizers.
\newline \newline
It works by using a shadow memory to map the memory regions allocated by the application and record whether each byte of such areas can be safely accessed by load/store operations. Each memory region is assigned a shadow counterpart, containing metadata about its size and the offset range that can be used to safely access it, while any attempt to read/write beyond such boundaries will trigger a sanitizer error, as shown by the figure below:
\newline
\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{foto/shadow_memory.png}
\caption{Memory mapping of ASan \cite{serebryany2012addresssanitizer}}
\label{fig:asan_shadow}
\end{figure}
\newline
Detection of out-of-bounds accesses to globals and stack objects is done in a similar way, which means by creating poisoned memory regions around such objects: \textit{global variables} are poisoned at compile-time and their addresses computed during the application startup, while \textit{stack objects} are poisoned and recorded at run-time.
\newline \newline
The management of the shadow memory is done by ASan run-time library, containing specialized implementation of the \verb|malloc| and \verb|free| functions, which allocate extra memory for shadow and poisoned memory zones as well as keeping a FIFO stack of allocated and freed memory regions to detect use-after-free, double-free and invalid-free bugs.
\newline \newline
Finally, ASan adds a slight overhead, increasing execution times by an average 170\% and memory usage by 3.4x.
\newline \newline \newline
Another component is the \textit{Leak Sanitizer} (or \textit{LSan}), a memory leak detector enabled by default in ASan that returns which portions of the program are leaking memory as well as the size leaked, thanks to comprehensive and exhaustive stack traces. Although this tool may seem very helpful, it usually generate huge logs especially when having complex programs that heavily rely on external libraries, which oftentimes do not perform clean releases of the objects used.


\newpage
\subsection{MSan}
The \textit{Memory Sanitizer} \cite{stepanov2015memorysanitizer} is a memory reads detector for C/C++ that helps developers to find and fix use-of-uninitialized-memory (UUM) bugs.
The C and C++ languages usually create uninitialized stack and heap objects (unless \verb|calloc| is invoked), and discovering these bugs can be quite tricky as they do not necessarily occur in every execution and could be triggered by any operation performed by the program.
\newline \newline
It works by using a shadow memory to map each bit of application memory and encode its state (0 initialized, 1 otherwise): all newly allocated memory is poisoned, i.e. the corresponding shadow memory is initialized to \verb|0xFF|, and "shadow propagation" operations are performed to safely copy an uninitialized variable between memory regions as well as simple logic and arithmetic operations without occurring in errors.
\newline
Performing a load operation from uninitialized memory returns an \textit{undefined value}, and operations like conditional branch, syscall and pointer dereference are most likely to trigger UUM bugs.
\newline
\begin{figure}[h]
\centering
\includegraphics[scale=0.73]{foto/shadow_memory_2.png}
\caption{Memory mapping of MSan \cite{stepanov2015memorysanitizer}}
\label{fig:msan_shadow}
\end{figure}
\newline
Given that UUM bugs are notoriously hard to reproduce and debug, the "origin-tracking-mode" can be used to obtain more comprehensive stack-traces and helping the developer to understand where it should have happened, although it relies on the program's symbols to provide meaningful information.
\newline
If necessary, there is also an "advanced-origin-tracking-mode", printing the stack traces of all operations from the allocation of the uninitialized variable up to its load, but its usability is still discussed due to the huge amount of memory it requires.
\newline \newline
The management of the shadow memory is done by MSan run-time library, that maps the "shadow" and (optional) "origin" areas and marks as uninitialized any new allocated regions as well as the deallocated ones. To update the shadow region, a large subset of the standard \verb|libc| functions are intercepted.
\newline \newline
Fixing UUM bugs is generally a quick task, like initializing the memory region using appropriate functions or simply setting all bytes to 0, but many developers avoid this sanitizer due to the huge slowdown that it introduces.
\newline
In fact MSan adds a modest overhead, increasing execution times by an average 300\% and memory usage by 2x on short programs, values that rapidly worsen with complex programs and when "origin-tracking-mode" is enabled.


\ \\
\subsection{UBSan}
The \textit{Undefined Behavior Sanitizer} \cite{ubsan_docs} is a fast undefined-behavior detector for C/C++, that helps developers to find and fix undefined-behavior (UB) bugs.
\newline
Common operations that may lead to undefined behaviors are: array subscription out of bounds, overflows/underflows originating from mathematical or logical operations, dereferenced, misagnlied or null pointers and conversion between data types.
\newline \newline
By default, due to its simplistic nature, the tool does not print stack-traces and uses a minimal run-time library. If needed, "print-stacktrace-mode" can be enabled to obtain more comprehensive results, although it relies on the programâ€™s symbols to provide meaningful information.
\newline \newline
Finally, UBSan adds a trivial overhead, increasing execution times by an average 120\% and memory usage by 2x.




\newpage
\section{Open-Source Software}
\matteo{I'm not sure this section belongs here in the first place. This is a master's thesis in computer science eng., I'm pretty sure we all know what open source is. Maybe just say something when you talk about OSS-Fuzz?}
\ziosaba{TBH, I'm not sure how to tackle this section, it started as something of just few lines but later proceeded to be a lot more that I originally intended. I didn't really like how the page looked with few lines on open-source immediately followed by Google Fuzzing, but I think that some of the problems of open-source discussed at the end of this section may be linked with OSS-Fuzz scoring methodology. Maybe instead of removing it altogether I can rewrite it to be more concise in a maximum of 1 page?}



The \textbf{Open-Source Software (OSS)} is a computer software developed in a collaborative and public manner, released under a particular license that allows other users to freely use, study, modify and distribute the software and its source code for any purpose: this allows many users to actively participate in the development of a software by proposing changes and new improvements.
\newline \newline
To be eligible as an open-source software, the license's distribution terms must comply with the following criteria: \cite{osd}
\begin{enumerate}
    
    \item \textbf{Free redistribution} \newline 
    The license must not restrict anyone from selling or giving away the software as part of a suite, nor it could be used to require royalties or fees on such sales.
    
    \item \textbf{Source code} \newline
    The program must include its source code, provided in a form that allows other programmers to modify it, as well as a compiled version. If the source code cannot be distributed, it should be easily obtainable thanks to well-publicized means, ideally downloadable from the Internet free of charge.
    It is not allowed to provide obfuscated source code or any partially-compiled form.
    
    \item \textbf{Derived works} \newline
    The license must allow for modifications and publishing of derived works, allowing them to be distributed under the same license of the original software
    
    \item \textbf{Integrity of the author's source code} \newline
    If the developers want to protect the original source code, they must allow the distribution of "patch files" to perform modification of the program at compile time. In this case, the license must explicitly allow distribution of software built from a modified source code as long as any derived works carry a different name or version number with respect to the original software. 
    
    \item \textbf{No discrimination against person or groups} \newline
    The license must not discriminate against any person or group of persons.
    
    \item \textbf{No discrimination against fields of endeavor} \newline
    The license must not restrict anyone from using the program in a particular field of endeavor or work.
    
    \item \textbf{Distribution of license} \newline
    The license's rights provided must apply for anyone that obtains the product, whether it is the original software or a redistributed version of it.
    
    \item \textbf{License must no be specific to a product} \newline
    The license's rights must not depend on the program being part of a suite. If that is true, the license of such suite must follow the same rights as those granted with the original software distribution. 
    
    \item \textbf{License must not restrict other software} \newline
    The license must not put restrictions of any other software that might be distributed along with the licensed software.
    
    \item \textbf{License must be technology-neutral} \newline
    No license provision may be linked to a particular technology or interface style.
\end{enumerate}
There are some key points that should be considered during the development of open-source software.
\newline \newline
First, the authors must decide how the program will be developed.
\newline
Usually, this software is released under two development branches: a "stable" version, composed by all the functionalities that have been thoroughly tested and work as intended, and a "build" version, that is slightly buggier as it includes proposed changes and new features that have yet to be refined.
Releasing the "build" version early not only allows the developers to showcase their work and attract even more new users, but also provides them with feedback from real users that are willing to run untested versions of their software.
\newline \newline
Then, they must decide how they will interact with online users.
\newline
In this sense, providing full access to the source code means that other users can submit new additions to the software, bug reports and code fixes as well as point out mistakes in the documentation, therefore helping the original developers in their works while also improving and refining the product. Moreover, given that each user may have different knowledge and programming skills as well as different testing environments, this allows to test and benchmark the product on a wide range of systems further increasing the probabilities of finding new and unknown bugs that may be specific to a single OS or architecture.
\newline \newline
Finally, it is important to mention that although any user has the rights to mention a bug, error, or mistake in the program, it is still up to the developers to ensure the truthfulness of what has been reported and how to tackle it.
\newline
For example, bugs that are not security-relevant or that may be related to QoL aspects are easily pushed back as secondary problems or simply ignored altogether.
Sometimes, if the developer are kind enough to accept your request but do not have time and resources to solve it, they might ask the user themselves for a proposed fix and cite them in the next patch notes as a way of thanking them.
\newline \newline
As will be discussed in future sections, while many reports produced in this work highlighted several security bugs that have been fixed in short times, some have been ignored due to them being not relevant at the moment of reporting or because they were declared as an incorrect building approach and/or use of the program itself.   
\newline \newline \newline
Given all this, one could argue that providing complete access to the source code and allowing other people to suggest changes poses a real threat to the security of the program.
\newline
History has shown us many times that, given enough time and resources, releasing the source code of a program will result in malicious users discovering bugs and vulnerabilities that could have potentially catastrophic consequences. Moreover, a malicious user might try to suggest a modification in the code that introduces a specific vulnerability or generate a bug report containing false information with attachments that might exploit a previously unknown vulnerability.
\newline
This is why having a large users base is important and one of the main advantages of open-source: if many people are closely watching how the program is being developed, one of them will most likely realize that malicious modifications are being suggested and notify others of the situation.
\newline \newline
Few noticeable mentions (LibreOffice, VLC, Firefox, etc...) ???




\newpage
\section{Continuous Fuzzing}
\matteo{we should call this "Continuous Fuzzing" and introduce Google's role in the description.}
\ziosaba{Google's role meaning the fact that it provides its own infrastructure for fuzzing and how it allows (virtually) any open-source project to enter this campaign?}

The \textit{Google Open Source Project} \cite{google_oss} is a campaign started in 2004, one of the oldest open-source campaigns in the industry. 
\newline
It was initially meant to share Google-developed software under open licenses, with the intention of bringing free technology and information sharing to the public, but it quickly became a program dedicated to improving open-source ecosystems as a whole. 
\newline \newline
Thanks to this campaign, many projects became popular and gained worldwide recognition such as Android OS, TensorFlow, the Go programming language, and many more.

\ \\
\subsection{ClusterFuzz}
The \textit{ClusterFuzz Project} is a scalable fuzzing infrastructure with the objective of discovering security and stability issues in software, it is the main platform used by Google to test its own products and also the fuzzing back-end for \textit{OSS-Fuzz}.
\newline
As of May 2023, it discovered over 25.000 bugs in Google proprietary software (e.g. Chrome) and 36.000 bugs with OSS-Fuzz. \cite{clusterfuzz_docs}
\newline \newline
It is based on a highly scalable distributed system of VMs, performing fully automatic bug filing, triage and closing as well as performance reports.
\newline \newline
\begin{figure}[h]
\makebox[\textwidth][c]{\includegraphics[width=0.665\paperwidth]{foto/clusterfuzz_architecture.png}}
\caption{ClusterFuzz main architecture visualized \cite{clusterfuzz_docs}}
\label{fig:clusterfuzz_architecture}
\end{figure}
\ \\
All operations are performed by two components.
\newline \newline
The \textit{App Engine} provides a web interface to the information collected during each fuzzing session, allowing the developers to easily access crashes, results and other information. This is also where tests can be scheduled, which is done via \verb|cron| jobs.
\newline \newline
The \textit{Fuzzing Bots Pool} is a cluster of VMs responsible for running the scheduled fuzzing sessions, and they perform the following operations:
\begin{itemize}
    \item \textbf{fuzz:} runs a fuzzing session
    \item \textbf{progression:} checks if a testcase still reproduces or if has been fixed
    \item \textbf{regression:} calculates the revision range in which a crash was introduced
    \item \textbf{minimize:} eliminates duplicate testcases from the input seeds
    \item \textbf{pruning:} minimize a corpus to the smallest size based on coverage information
    \item \textbf{analyze:} runs a manually uploaded testcase against a specific job to see if it crashes
\end{itemize}
\ \\
Given that some of this tasks are critical and should be treated as atomic operations, bots can be \textit{preemptible} or \textit{non-preemptible}.
\newline
The first refers to a machine that can only run the "fuzz" task as it can be shut down at any moment.
\newline
The latter refers to a machine that is not expected to abruptly stop or crash, therefore is capable of running all tasks.
\newline \newline \newline
Each VMs performs these operations inside Docker instances, created and provided by the developer using Dockerfiles, that are configured with all the tools and files necessary to correctly build and launch the fuzzing targets.


\ \\
\subsection{OSS-Fuzz}
The \textit{OSS-Fuzz Project} \cite{ossfuzz_paper} was created in 2016 after the famous "Heartbleed" vulnerability was discovered in OpenSSl, one of the most popular open-source projects at the time for encrypting web traffic, as a response to provide developers with free fuzzing and private alerts services for their open-source projects.
While it was initially intended for languages that are not memory-safe (C/C++), it is now capable to provide support for other popular languages such as Python, Go, Java and Rust.  
\newline
As of August 2023, it helped identify and fix over 10.000 vulnerabilities and 36.000 bugs across over 1000 projects. \cite{ossfuzz_docs}.
\newline \newline
Projects can be tested using several fuzzing engines (such as LibFuzzer, AFL++, Honggfuzz and Centipede) in combination with Google Sanitizers (ASan, MSan and UBSan), while \textit{ClusterFuzz} acts as the back-end and reporting tool.


\newpage
\begin{figure}[h]
\makebox[\textwidth][c]{\includegraphics[width=0.65\paperwidth]{foto/oss-fuzz_architecture.png}}
\caption{OSS-Fuzz main architecture visualized \cite{ossfuzz_docs}}
\label{fig:ossfuzz_architecture}
\end{figure}
\ \\
Initially, the maintainer of an open-source project creates one or more "fuzz targets" that will be integrated with the project's build and test systems. \cite{libfuzzer_docs}
\newline
A "fuzz target" is essentially a function that accepts an input, in this case an array of bytes, and perform some operations with these bytes to test a specific API.
\newline
Although not all projects are expected to implement and maintain their targets in the same way, developers can refer to a guide of recommendations to increase the efficiency and quality of the automated fuzzing tests performed.
\newline \newline
To briefly summarize them, they should:
\begin{itemize}
    \item maintain the source code and targets' build system using some versioning service (like Git)
    \item allow each fuzz target to be compiled with Sanitizers
    \item avoid modular build systems for the fuzz targets (compile all or nothing) and use general-scope compile flags
    \item provide a seed corpus that is regularly updated and extended with new "interesting inputs", also it should have good coverage
    \item provide a dictionary of tokens to instruct the fuzzer on the correct syntax of the inputs, if applicable
    \item periodically check the performances achieved by each fuzz target, meaning their coverage, time spent on execution and solving abrupt errors
\end{itemize}


\newpage
To be accepted by OSS-Fuzz, a project must be relevant and/or critical to the global IT infrastructure and it must have a significant user base.
\newline \newline
To check if your project is eligible for OSS-Fuzz, Google provides a mathematical formula to calculate the "Open-Source Project Criticality Score" \cite{score} in a range between 0 \textit{(least critical)} and 1 \textit{(most critical)}:
\ \\ \newline
\begin{equation}
    CriticalityScore = \frac{1}{\sum \alpha_i}\  \sum_i \alpha_i \ \frac{\log(1+S_i)}{\log(1+\max(S_i,T_i))}
\end{equation}
\ \\ \newline
where $\alpha_i$ refers to the "weight" of an indicator and $T_i$ its maximum allowed value.
\newline \newline \newline
The parameters evaluated are the followings:
\begin{itemize}
    \item \textbf{created-since} ($\alpha_i = 1$, $T_i = 120$): months since the project was created, older projects have higher chances of being widely used or being dependent upon
    \item \textbf{updated-since} ($\alpha_i = -1$, $T_i = 120$): months since the project was last updated, unmaintained projects with no recent commits have higher chance of being less relied upon
    \item \textbf{contributor-count} ($\alpha_i = 2$, $T_i = 5000$): count of commits made by project contributors, different contributors involvement indicates project's importance
    \item \textbf{org-count} ($\alpha_i = 1$, $T_i = 10$): count of distinct organizations contributing to the project, indicates cross-organization dependency
    \item \textbf{commit-frequency} ($\alpha_i = 1$, $T_i = 1000$): average number of commits per week in the last year, code that changes constantly means higher susceptibility to vulnerabilities
    \item \textbf{recent-releases-count} ($\alpha_i = 0.5$, $T_i = 26$): number of releases in the last year, frequent releases indicates user dependency
    \item \textbf{closed-issues-count} ($\alpha_i = 0.5$, $T_i = 5000$): number of issues closed in the last 90 days, indicates contributor involvement and focus on closing user issues
    \item \textbf{updated-issues-count} ($\alpha_i = 0.5$, $T_i = 5000$): number of issues updated in the last 90 days, indicates high contributor involvement
    \item \textbf{comment-frequency} ($\alpha_i = 1$, $T_i = 15$): average number of comments per issue in the last 90 days, indicates  user activity and dependence
    \item \textbf{dependents-count} ($\alpha_i = 2$, $T_i = 500000$): number of project mentions in the commit messages, indicates repository use
\end{itemize}
\ \\
Only projects scoring a value $\geq 0.7$ may be eligible to be integrated in the OSS-Fuzz campaign.
\newline \newline



\newpage
Assuming your project satisfies the score requirements, developers issue a "pull request" on the OSS-Fuzz repository providing the following 3 files.
\newline \newline
The \textit{project.yaml} configuration file stores project metadata needed by OSS-Fuzz to correctly provide its services, and some of the most commons are:
\begin{itemize}
    \item \textbf{homepage:} url to the project's homepage
    \item \textbf{language:} programming language used to write the project (C, C++, Go, Rust, Python, JVM languages, Swift)
    \item \textbf{primary\_contacts:} list of email addresses that will be automatically CCed on crash reports and fuzzer statistics
    \item \textbf{main\_repo:} path to the source code repository hosting the project
    \item \textbf{vendor\_ccs:} \textit{optional}, list of vendors' email addresses that want access to the bug reports
    \item \textbf{sanitizers:} \textit{optional}, list of sanitizers that are available on the project (address, undefined, memory), if not specified "address" and "undefined" will be used
    \item \textbf{architectures:} \textit{optional}, list of architectures that can build the project (supported only x86\_64 and i386)
    \item \textbf{fuzzing\_engines:} \textit{optional}, list of fuzzing engines available (libfuzzer, afl, honggfuzz and centipede), if not specified "libfuzzer" will be used
    \item \textbf{builds\_per\_day:} \textit{optional}, number of times a project should be built per day, OSS-Fuzz allows up to 4 builds per day and builds once per day by default
\end{itemize}
\ \\
The \textit{Dockerfile} defines a container with all the dependencies and resources needed to build the project and prepare the testing environment.
\newline
To avoid having developers create and maintain their own customized Docker containers and create an homogeneous and standard environment, OSS-Fuzz provides several "base images" for each supported language, built on top of Ubuntu 20.04 and with the appropriate compilers and toolchains already installed.
\newline
After pulling the most appropriate base image, there are usually several "apt-get" and "git clone" commands related to downloading and installing all the required packages and dependencies needed to correctly build the project.
\newline
Finally, the build script and any other fuzzer files are pulled from the project's repository.
\newline \newline
The \textit{build.sh:} script, executed inside the Docker container, performs all the operations necessary to build the project and the fuzzing targets that will be tested.
\newline
In general, the script should build the project and the fuzz targets using the correct compiler and appropriate compiler flags.
\newline
To provide a more flexible and accessible build system, the Docker image provides several environment variables regarding directory locations, compilers and compiling flags, allowing developers to easily re-target scripts with little effort.
\newline \newline \newline
On the other end, a bot periodically checks for new requests and validates their content before accepting/rejecting them.





\newpage
Once the project has been accepted as part of the OSS-Fuzz's infrastructure, a "builder" script follows the provided instructions to build the project's fuzz targets and uploads them to a Google Cloud Service Bucket, a file-hosting service.
\newline
This acts as a middle point between OSS-Fuzz and ClusterFuzz, which uses the aforementioned bucket to download all the necessary elements to fuzz the project as well as upload the results achieved.
\newline \newline \newline
After a successful fuzzing session, any bug discovered is reported to the OSS-Fuzz issue tracker \cite{ossfuzz_bugtracker}, which uses the metadata sent by ClusterFuzz to create its report. 
\newline
Developers have 3 ways of dealing with this situation: they can commit new changes to fix the bug (verified by ClusterFuzz before closing an issue), assign the tag "WontFix" to the bug to notify that it will not be solved, or simply ignore it altogether. 
\newline \newline \newline
Finally, OSS-Fuzz follows a strict \textit{bug disclosure guideline}. \cite{bug_disclosure}
\newline \newline
When a bug is discovered, an automatic email is generated and sent to all email addresses specified in the project, and an issue is opened on the issue tracker.
\newline
This email contains the report created using ClusterFuzz, as well as an estimation of the priority and severity of the bug discovered.
\newline \newline
From this moment, the issue will be publicly visible in 90 days or after the fix is released (whichever comes earlier), meaning that anyone will have access to the causing input as well as any other debugging information related to what happened and how to reproduce the bug.
\newline
Before the deadline expires, the developers may request a 14-day grace period if the patch is set to be released on a specific day within this extended period, in which case the public disclosure is delayed.
\newline \newline
In any case, Google reserves the right to change deadlines forwards or backwards depending on the circumstances and the severity of the findings.


\ \\
\subsection{FuzzBench}
The \textit{FuzzBench Project} \cite{fuzzbench_paper} is a free service that provides fuzzers' developers with several real-world benchmark tested at Google scale, comparing the results with other famous fuzzers (such as AFL and LibFuzzer) and allowing them to evaluate their performances thanks to daily reports for further improvements.
\cite{fuzzbench_docs}
\newline
\begin{figure}[h]
\makebox[\textwidth][c]{\includegraphics[width=0.65\paperwidth]{foto/fuzzbench_architecture.png}}
\caption{FuzzBench main architecture visualized \cite{fuzzbench_docs}}
\label{fig:fuzzbench_architecture}
\end{figure}
\ \\
Initially, a fuzzer developer integrates its product within FuzzBench using a Dockerfile, containing all the resources necessary to build targets using the fuzzer and also where all benchmarks will be executed.
\newline
Similarly to OSS-Fuzz, this process is done via "pull requests", which are automatically revisioned and accepted by bots.
\newline \newline
Then, the developers may choose between two testing approaches: standard and OSS-Fuzz.
\newline
In the first case, the benchmark is created by the developers themselves, and this requires the definition of fuzz targets, build files and Docker images that will be used to correctly build and link the fuzzer to the targets and run the tests.
\newline
In the latter, the developers employ a fuzz target from any OSS-Fuzz project as benchmark, allowing them to test their product on a real-world scenario.
\newline \newline
Finally, a report will be created highlighting the strengths and weaknesses of the fuzzer on the various benchmarks, comparing individual and overall results with other integrated fuzzers.

\ \\ \newline \newline \newline
Explain what is a project in FuzzBench...
\newline \newline
Explain the structure of a project and its files...
\newline \newline
Explain the commands from the "gsutil" suite...
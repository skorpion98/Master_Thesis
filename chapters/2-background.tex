\chapter{Background}

\matteo{Use this space to introduce the chapter. "In this chapter we will provide the necessary background for this thesis. We will start by introducing how xxx works, with an in-depth analysis of its yyy component and its effects on zzz. Then, we will discuss www [...]. Finally, we will conclude by detailing the inner working of vvv [...]}

\matteo{Push all this text in the Fuzz Testing section. You are introducing fuzz testing here and then you start the fuzz testing section with much information already given.}

\matteo{Once you push this text below, be careful not to repeat the same concept too many times.}

In the context of software development, \textit{fuzz testing} (or \textit{fuzzing}) refers to an automated software technique which focuses on providing random data to a program, with the objective of creating invalid or unexpected inputs that may potentially trigger crashes, assertions or memory leaks.
\newline 
This also allows developers to test their programs for so called "corner-cases", meaning situations that are hard or complex to reproduce as they should not occur when the program is being properly used, but still that could lead to unexpected and/or potential malicious behavior and thus should be properly handled. 
\newline \newline
Most fuzzers take structured inputs as reference, that will be used to generate inputs that are "valid enough" to be accepted by the program, but the strategies applied to generate such new inputs can heavily influence the effectiveness of the tests as well as the code-coverage achieved.
\newline
For this reason, fuzzers can be categorized using the 3 following characteristics:
\begin{itemize}
    \item how new inputs are generated (\textit{generation-based} or \textit{mutation-based})
    \item whether they are aware of the input structure (\textit{smart fuzzers}) or not (\textit{dumb fuzzers})
    \item whether they are aware of the program structure (\textit{white-box}), partially aware (\textit{gray-box}) or not (\textit{black-box})
\end{itemize}
It's important to mention that there is a trade-off between results, time spent testing and resources available.
\newline
Fuzzing is a technique that is quite resource-intensive, putting the CPU cores under heavy load while also generating potentially enormous log files, and these tests are usually run for several hours or even days, but preparing the right set of inputs that will be used to perform the tests is crucial to obtain good results.
\newline
Moreover, during a fuzzing session, there will be a point in time where either the inputs generated by the fuzzer will not trigger new execution flows or no new bugs are discovered for quite some time: reaching this state does not mean that the program is finally free of other bugs, as the conditions to trigger them might simply have not happened yet, and it's obviously impossible to know when this will happen as bugs may be discovered randomly.
\newline \newline \newline
Among the many results achieved thanks to this technique, two honorable mentions can be linked to the popular fuzzer \textit{American Fuzzy Lop} (also known as \textit{AFL} \cite{AFL}).
\newline
In September 2014, AFL was used to discover "Shellshock" \cite{shellshock} (also known as "Bashdoor"), a family of security bugs that affected the Unix Bash shell allowing malicious users to execute arbitrary commands without confirmation.
\newline
In April 2015, AFL discovered the famous "Heartbleed" \cite{heartbleed} bug in OpenSSL, which allowed malicious users to decipher the otherwise encrypted communication from the TLS protocol. 
\newline \newline
This thesis will focus on two campaigns maintained by Google called \textit{OSS-Fuzz} \cite{doc-ossfuzz} and \textit{FuzzBench} \cite{{doc-fuzzbench}}: the first is a free platform that allows open-source developers to fuzz their programs autonomously relying on the computing resources provided by the Google Cloud service, while the latter allows fuzzer developers to test and improve their tools on real-word benchmarks thanks to automated tests and periodic reports.
\newline \newline
More specifically, it revolves around fuzzing some of the projects that have been implemented in these repositories using alternative approaches, trying to discover new and relevant bugs that will be then securely reported and disclosed to their developers in hope to have them fixed.

\newpage
\matteo{Don't use random newpages and newlines; let sapthesis do its things. Use double newlines (intendo il tasto invio, non il comando) instead of the command newline, or are you using it for some specific reason?}
\section{What is Fuzz Testing}
\matteo{I'd call this section "Fuzz Testing" and I would merge 2.2 as a paragraph of this section (or maybe a subsection, you choose).}
As previously mentioned, \textit{fuzz testing} is a software testing technique that feeds invalid, random and unexpected data to a program with the objective of discovering inputs whose execution may lead to crashes, failing assertions and memory leaks.
\newline \newline
We mainly distinguish between 3 types of fuzz testing:
\begin{itemize}
    \item \textbf{application fuzzing:} used for UI elements (such as buttons, input fields) or command-line programs, tests may include high-frequency inputs, providing random/invalid content and inputs exceeding the expected size
    \item \textbf{protocol fuzzing:} used to test the behavior of network elements and servers when invalid messages are sent over a chosen protocol, useful to ensure that such content is not misinterpreted and potentially executed as commands
    \item  \textbf{file format fuzzing:} used for programs that accept "structured inputs", meaning files that have a precise and standard format (like .doc, .jpg) which the fuzzer can modify with the intention of triggering unwanted behavior
\end{itemize}
This work will focus on testing programs that accept input files to provide their services, and for this reason several corpora were used to instruct the fuzzer on the appropriate file format. This will be discussed more in depth in section \ref{fuzzers}.
\newline \newline
One of the main requirements in fuzz testing is to achieve a high degree of \textbf{code coverage}: this statistic measures the percentage of source code executed in a test session, therefore achieving a high value means lowering the chance of leaving parts of the program with undetected bugs. Given this, one could argue that the fuzzer should be aware of the program's structure to maximize the coverage, but given that this process requires a non-trivial overhead it comes with its own trade-offs.
\newline \newline
For this reason, we define 3 different approaches.
\newline \newline
The \textit{black-box testing} involves using a fuzzer that is completely unaware of the program's structure, therefore assumes the program as a simple machine that takes a random input and generates an according output. This approach is relatively fast, can be easily parallelized and has good scalability. On the other hand, it will most likely find only "surface" bugs, i.e. they do not require particular conditions to be met to be triggered.     
\newline \newline
The \textit{white-box testing} involves using a fuzzer that employs "program analysis" techniques to systematically explore and reach critical program locations through meticulously crafted inputs, allowing you to discover bugs that could be potentially hidden deep in the program, although heavily relying on this knowledge implies that bugs related to unknown aspects of the program can be easily missed. While this approach is arguably the most effective one, the time used to analyze the program as well as generating such specialized input exponentially increases with the program complexity.
\newline \newline
The \textit{gray-box testing} attempts to integrate the best aspects of both approaches: use a minimal amount of knowledge over the program's structure to achieve a sufficient degree of code coverage such that the results obtained are satisfactory. This is usually done thanks to "instrumentation", discussed in section \ref{sanitizers}.
The previously mentioned AFL, which has been extensively used in this work, falls in this category.

\newpage
A fuzzing session usually yields two outputs: a set of "interesting" inputs and a list of potential bugs.
\newline \newline
Whenever a generated input results in a new execution flow, it is saved along with others "interesting inputs", which can be used in subsequent fuzzing sessions to provide the fuzzer with even more information about the structure of a good input that explores the deepest parts of the program. Then, the developer might decide to analyze and group these lists, to create a new one that does not contain duplicates and/or inputs triggering the same execution flows, which is particularly useful to ensure that the size does not explode over time.
\newline \newline
Regarding the discovering of bugs, a fuzzer has to be sensible enough to distinguish between crashing and non-crashing inputs without having full knowledge over the program tested, and "sanitizers" are used to inject assertions that make the program crash when a particular kind of failure is detected. Section \ref{sanitizers} explains this concept more thoroughly.
\newline
Then, the fuzzer might produce one (or more) inputs that triggered different kind of bugs, and the developer has to perform what is called \textit{bug triage}:
\begin{itemize}
    \item execute each input individually and observe the output
    \item determine which kind of error occurred and why, maybe also introducing debugging tools
    \item fix the bug entirely, if possible, or at least patch the problem as much as possible
    \item ensure that the bug does not occur in future fuzzing sessions by including the triggering input(s) in the set used for the tests
\end{itemize}
\ \\
Finally, it's important to recall that running a fuzzing session for several days or weeks without bugs does not necessarily imply that the program is secure: this process is driven by randomness, initial inputs provided and environment used, meaning also that each fuzzing session will always result in slightly different results and coverage achieved. Yet, testing a program on all possible inputs is obviously impossible.
\newline
Moreover, it easily generates false positives (trivial or benign warnings) and false negative results (incorrect or misleading results), further overloading the already tedious work of manually assessing each bug.

\ \\ \newline \newline
\newline \newline
Explain the main shortcomings of fuzz testing (maybe also available solution to mitigate them???)...



\newpage
\section{Different types of Fuzz Testers} \label{fuzzers}
A fuzzer is composed by the following key elements. \cite{afl_docs}
\newline \newline
\textbf{Observer.}\ \ \ Provides information observed during the execution of a program to the fuzzer. Such information may be relatively simple, like the total running time for a single test and its output, to more advanced ones, like the maximum depth of the stack in a single execution. They are usually not preserved across many execution, unless an "interesting input" is encountered, in which case they are relayed to other nodes to improve future fuzzing sessions. 
\newline \newline
\textbf{Executor.}\ \ \ Responsible for defining how the program will be executed and the arguments passed on each run. The input for a single test is provided either by writing it in some specific memory location or passed as argument to a so called "harness function", although each fuzzer has its own implementation of this element. Given this, we briefly mention few standard functionalities that compose this element. The \textit{InProcessExecutor} runs the "harness function" and provides crash detection. The \textit{ForkServerExecutor} is responsible for spawning different child processes to fuzz. The \textit{TimeoutExecutor} wraps and installs a timeout for another running executor.
\newline \newline
\textbf{Feedback.}\ \ \ Classifies the result of a single execution and determines if the initial input is "interesting" or not, which is usually determined by retrieving information for the observers and analyzing the updated coverage map. It's also possible to define several of these elements, each one with its own objective (crashes, timeout, new execution flow discovered), and combine them in boolean expression to collect more fine-grained results.
\newline \newline
\textbf{Input.}\ \ \ Data taken from an external source and provided to the tested program to observe its behavior, usually in the form of bytes arrays. The first fuzzing session takes a set of inputs that is defined and provided by the developer itself, while future fuzzing session will also rely on previously discovered "interesting inputs". 
\newline \newline
\textbf{Testcase.}\ \ \ Defined as an input and a set of related metadata, like ID, description and expected results.
\newline \newline
\textbf{Corpus.}\ \ \ Location where testcases are stored, usually disk or memory. An example of input corpus may be composed by several testcases with the same properties, like crashing the program under a specific situation. An example of output corpus may be composed by all the testcases that are considered "interesting".
\newline \newline \newline
This thesis focused on using fuzzers capable of generating new structured inputs for the tested program starting from an initial \textit{seed}, specifically files with a precise format, allowing them to distinguish between a valid and an invalid example. 
\newline
However, even files that do not necessarily follow the defined structure can still be used as input for fuzzing, maybe with an even higher chance of triggering unwanted or unexpected behavior: for this reason, an effective fuzzer should be capable of generating inputs that are "valid enough" so that they are not rejected from the program's parser, and "invalid enough" to potentially trigger corner cases.


\newpage
We finally define the two main types of fuzzers.
\newline \newline
The \textbf{mutational-based fuzzers} require a corpus of seed inputs as reference, and generate new inputs by applying "mutators" on the provided seeds: these operations range from flipping single bits or bytes on a given input or performing mathematical operations over them, to adding or removing said information, sometimes even completely randomizing its content. Furthermore, these mutation may be mixed-and-matched into long sequences, to generate even more new inputs.
\newline
A \textit{smart mutational fuzzer} might leverage its knowledge on the input model to switch between different types of inputs, although this information is not always available.
\newline
A \textit{dumb mutational fuzzer}, like AFL, employs random mutations using as reference the content of "interesting inputs", which usually results in a much lower proportion of valid inputs generated.
\newline \newline
The \textbf{generation-based fuzzers} generate new inputs from scratch, usually relying on a good source of randomness to perform this operation, and for this reason they do not depend on the existence of a good corpus nor its quality.
\newline
A \textit{smart generation fuzzer} might leverage the input model provided by the developer to generate valid new inputs, although this information may not always be available.
\newline
A \textit{dumb generation fuzzer} attempts to generate new inputs without any reference, oftentimes putting more stress on the program's parser rather than the program itself, as they generate an overwhelmingly amount of invalid inputs.



\section{Sanitizers}
\matteo{We are missing a section on sanitizers (I just added it). Introduce them, by also highlighting the differences between compiler ones (ASan, MSan, UBSan...) and binary ones (mainly Valgrind and DrMemory). All these things have been presented in papers, be sure to cite them, don't cite their sites if there are papers.}

\matteo{I see you talk about sanitizers later, I would do it here; it also enables you to talk about them when talking about OSS-Fuzz and FuzzBench.}



\newpage
\section{Open-Source Software}
\matteo{I'm not sure this section belongs here in the first place. This is a master's thesis in computer science eng., I'm pretty sure we all know what open source is. Maybe just say something when you talk about OSS-Fuzz?}
The \textbf{Open-Source Software (OSS)} is a computer software developed in a collaborative and public manner, released under a particular license that allows other users to freely use, study, modify and distribute the software and its source code for any purpose: this allows many users to actively participate in the development of a software by proposing changes and new improvements.
\newline \newline
To be eligible as an open-source software, the license's distribution terms must comply with the following criteria: \cite{osd}
\begin{enumerate}
    
    \item \textbf{Free redistribution} \newline 
    The license must not restrict anyone from selling or giving away the software as part of a suite, nor it could be used to require royalties or fees on such sales.
    
    \item \textbf{Source code} \newline
    The program must include its source code, provided in a form that allows other programmers to modify it, as well as a compiled version. If the source code cannot be distributed, it should be easily obtainable thanks to well-publicized means, ideally downloadable from the Internet free of charge.
    It is not allowed to provide obfuscated source code or any partially-compiled form.
    
    \item \textbf{Derived works} \newline
    The license must allow for modifications and publishing of derived works, allowing them to be distributed under the same license of the original software
    
    \item \textbf{Integrity of the author's source code} \newline
    If the developers want to protect the original source code, they must allow the distribution of "patch files" to perform modification of the program at compile time. In this case, the license must explicitly allow distribution of software built from a modified source code as long as any derived works carry a different name or version number with respect to the original software. 
    
    \item \textbf{No discrimination against person or groups} \newline
    The license must not discriminate against any person or group of persons.
    
    \item \textbf{No discrimination against fields of endeavor} \newline
    The license must not restrict anyone from using the program in a particular field of endeavor or work.
    
    \item \textbf{Distribution of license} \newline
    The license's rights provided must apply for anyone that obtains the product, whether it is the original software or a redistributed version of it.
    
    \item \textbf{License must no be specific to a product} \newline
    The license's rights must not depend on the program being part of a suite. If that is true, the license of such suite must follow the same rights as those granted with the original software distribution. 
    
    \item \textbf{License must not restrict other software} \newline
    The license must not put restrictions of any other software that might be distributed along with the licensed software.
    
    \item \textbf{License must be technology-neutral} \newline
    No license provision may be linked to a particular technology or interface style.
\end{enumerate}
There are some key points that should be considered during the development of open-source software.
\newline \newline
First, the authors must decide how the program will be developed.
\newline
Usually, this software is released under two development branches: a "stable" version, composed by all the functionalities that have been thoroughly tested and work as intended, and a "build" version, that is slightly buggier as it includes proposed changes and new features that have yet to be refined.
Releasing the "build" version early not only allows the developers to showcase their work and attract even more new users, but also provides them with feedback from real users that are willing to run untested versions of their software.
\newline \newline
Then, they must decide how they will interact with online users.
\newline
In this sense, providing full access to the source code means that other users can submit new additions to the software, bug reports and code fixes as well as point out mistakes in the documentation, therefore helping the original developers in their works while also improving and refining the product. Moreover, given that each user may have different knowledge and programming skills as well as different testing environments, this allows to test and benchmark the product on a wide range of systems further increasing the probabilities of finding new and unknown bugs that may be specific to a single OS or architecture.
\newline \newline
Finally, it is important to mention that although any user has the rights to mention a bug, error, or mistake in the program, it is still up to the developers to ensure the truthfulness of what has been reported and how to tackle it.
\newline
For example, bugs that are not security-relevant or that may be related to QoL aspects are easily pushed back as secondary problems or simply ignored altogether.
Sometimes, if the developer are kind enough to accept your request but do not have time and resources to solve it, they might ask the user themselves for a proposed fix and cite them in the next patch notes as a way of thanking them.
\newline \newline
As will be discussed in future sections, while many reports produced in this work highlighted several security bugs that have been fixed in short times, some have been ignored due to them being not relevant at the moment of reporting or because they were declared as an incorrect building approach and/or use of the program itself.   
\newline \newline \newline
Given all this, one could argue that providing complete access to the source code and allowing other people to suggest changes poses a real threat to the security of the program.
\newline
History has shown us many times that, given enough time and resources, releasing the source code of a program will result in malicious users discovering bugs and vulnerabilities that could have potentially catastrophic consequences. Moreover, a malicious user might try to suggest a modification in the code that introduces a specific vulnerability or generate a bug report containing false information with attachments that might exploit a previously unknown vulnerability.
\newline
This is why having a large users base is important and one of the main advantages of open-source: if many people are closely watching how the program is being developed, one of them will most likely realize that malicious modifications are being suggested and notify others of the situation.
\newline \newline
Few noticeable mentions (LibreOffice, VLC, Firefox, etc...) ???




\newpage
\section{Fuzzing with Google}
\matteo{we should call this "Continuous Fuzzing" and introduce Google's role in the description.}
\matteo{Also, consider talking about clusterfuzz first and then OSS-Fuzz and Fuzzbench (which you did not include it here, maybe you wrote this before working on fuzzbench). OSS-Fuzz and Fuzzbench rely on clusterfuzz, so it makes sense to talk about clusterfuzz, I guess.}
The \textit{Google Open Source Project} \cite{google_oss} is a campaign started in 2004, one of the oldest open-source campaigns in the industry. 
\newline
It was initially meant to share Google-developed software under open licenses, with the intention of bringing free technology and information sharing to the public, but it quickly became a program dedicated to improving open-source ecosystems as a whole. 
\newline \newline
Thanks to this campaign, many projects became popular and gained worldwide recognition such as Android OS, TensorFlow, the Go programming language, and many more.


\ \\
\subsection{OSS-Fuzz}
The \textit{OSS-Fuzz Project} was created in 2016 after the famous "Heartbleed" vulnerability was discovered in OpenSSl, one of the most popular open-source projects at the time for encrypting web traffic, as a response to provide developers with free fuzzing and private alerts services for their open-source projects.
While it was initially intended for languages that are not memory-safe (C/C++), it is now capable to provide support for other popular languages such as Python, Go, Java and Rust.  
\newline
As of August 2023, it helped identify and fix over 10.000 vulnerabilities and 36.000 bugs across over 1000 projects. \cite{ossfuzz_docs}.
\newline \newline
Projects can be tested using several fuzzing engines (such as LibFuzzer, AFL++, Honggfuzz and Centipede) in combination with Google Sanitizers (ASan, MSan and UBSan), while \textit{ClusterFuzz} acts as the back-end and reporting tool.
\newline \newline
\begin{figure}[h]
\makebox[\textwidth][c]{\includegraphics[width=0.77\paperwidth]{foto/oss-fuzz_architecture.png}}
\caption{OSS-Fuzz main architecture visualized \cite{ossfuzz_docs}}
\label{fig:ossfuzz_architecture}
\end{figure}

\newpage
The process works as follows.
\newline \newline \newline
Initially, the maintainer of an open-source project creates one or more "fuzz targets" that will be integrated with the project's build and test systems. \cite{libfuzzer_docs}
\newline
A "fuzz target" is essentially a function that accepts an input, in this case an array of bytes, and perform some operations with these bytes to test a specific API.
\newline
Although not all projects are expected to implement and maintain their targets in the same way, developers can refer to a guide of recommendations to increase the efficiency and quality of the automated fuzzing tests performed.
\newline
To briefly summarize them, they should:
\begin{itemize}
    \item maintain the source code and targets' build system using some versioning service (like Git)
    \item allow each fuzz target to be compiled with Sanitizers
    \item avoid modular build systems for the fuzz targets (compile all or nothing) and use general-scope compile flags
    \item provide a seed corpus that is regularly updated and extended with new "interesting inputs", also it should have good coverage
    \item provide a dictionary of tokens to instruct the fuzzer on the correct syntax of the inputs, if applicable
    \item periodically check the performances achieved by each fuzz target, meaning their coverage, time spent on execution and solving abrupt errors
\end{itemize}
\ \\
Then, the newly prepared project must be accepted by OSS-Fuzz, which is done by issuing a "pull request" on the project repository with some requested information, such as: project's main repo, language used, building instructions and email addresses to contact on new issues.
\newline
On the other end, a bot periodically checks for new requests and validates their content before accepting/rejecting them.
\newline \newline \newline
Once the project has been accepted as part of the OSS-Fuzz's infrastructure, a "builder" script follows the provided instructions to build the project's fuzz targets and uploads them to a Google Cloud Service Bucket, a file-hosting service.
\newline
This acts as a middle point between OSS-Fuzz and ClusterFuzz, which uses the aforementioned bucket to download all the necessary elements to fuzz the project as well as upload the results achieved.
\newline \newline \newline
After a successful fuzzing session, any bug discovered is reported to the OSS-Fuzz issue tracker \cite{ossfuzz_bugtracker}, which uses the metadata sent by ClusterFuzz to automatically create a report. 
\newline
Developers have 3 ways of dealing with this situation: they can commit new changes to fix the bug (verified by ClusterFuzz before closing an issue), assign the tag "WontFix" to the bug to notify that it will not be solved, or simply ignore it altoghether. 
\newline \newline
These steps create a cycle that allows for continuous fuzzing and improvement of the software.


\newpage
Finally, OSS-Fuzz follows a strict \textit{bug disclosure guideline}. \cite{bug_disclosure}
\newline \newline
When a bug is discovered, an automatic email is generated and sent to all email addresses specified in the project, and an issue is opened on the issue tracker.
\newline
This email contains the report created using ClusterFuzz, as well as an estimation of the priority and severity of the bug discovered.
\newline \newline
From this moment, the issue will be publicly visible in 90 days or after the fix is released (whichever comes earlier), meaning that anyone will have access to the causing input as well as any other debugging information related to what happened and how to reproduce the bug.
\newline
Before the deadline expires, the developers may request a 14-day grace period if the patch is set to be released on a specific day within this extended period, in which case the public disclosure is delayed.
\newline \newline
In any case, Google reserves the right to change deadlines forwards or backwards depending on the circumstances and the severity of the findings.





\ \\
\subsection{ClusterFuzz}
The \textit{ClusterFuzz Project} is a scalable fuzzing infrastructure with the objective of discovering security and stability issues in software, it is the main platform used by Google to test its own products and also the fuzzing back-end for \textit{OSS-Fuzz}.
\newline
As of May 2023, it discovered over 25.000 bugs in Google proprietary software (e.g. Chrome) and 36.000 bugs with OSS-Fuzz. \cite{clusterfuzz_docs}
\newline \newline
It is based on a highly scalable distributed system of VMs, performing fully automatic bug filing, triage and closing as well as performance reports.
\newline \newline
\begin{figure}[h]
\makebox[\textwidth][c]{\includegraphics[width=0.665\paperwidth]{foto/clusterfuzz_architecture.png}}
\caption{ClusterFuzz main architecture visualized \cite{clusterfuzz_docs}}
\label{fig:clusterfuzz_architecture}
\end{figure}
\ \\
All operations are performed by two components.
\newline \newline
The \textit{App Engine} provides a web interface to the information collected during each fuzzing session, allowing the developers to easily access crashes, results and other information. This is also where tests can be scheduled, which is done via \verb|cron| jobs.
\newline \newline
The \textit{Fuzzing Bots Pool} is a cluster of VMs responsible for running the scheduled fuzzing sessions, and they perform the following operations:
\begin{itemize}
    \item \textbf{fuzz:} runs a fuzzing session
    \item \textbf{progression:} checks if a testcase still reproduces or if has been fixed
    \item \textbf{regression:} calculates the revision range in which a crash was introduced
    \item \textbf{minimize:} eliminates duplicate testcases from the input seeds
    \item \textbf{pruning:} minimize a corpus to the smallest size based on coverage information
    \item \textbf{analyze:} runs a manually uploaded testcase against a specific job to see if it crashes
\end{itemize}
\ \\
Given that some of this tasks are critical and should be treated as atomic operations, bots can be \textit{preemptible} or \textit{non-preemptible}.
\newline
The first refers to a machine that can only run the "fuzz" task as it can be shut down at any moment.
\newline
The latter refers to a machine that is not expected to abruptly stop or crash, therefore is capable of running all tasks.
\newline \newline \newline
Each VMs performs these operations inside Docker instances, created and provided by the developer using Dockerfiles, that are configured with all the tools and files necessary to correctly build and launch the fuzzing targets.





\ \\
\subsection{FuzzBench}
The \textit{FuzzBench Project} is a free service that provides fuzzers' developers with several real-world benchmark tested at Google scale, comparing the results with other famous fuzzers (such as AFL and LibFuzzer) and allowing them to evaluate their performances thanks to daily reports for further improvements.
\cite{fuzzbench_docs}
\newline
\begin{figure}[h]
\makebox[\textwidth][c]{\includegraphics[width=0.67\paperwidth]{foto/fuzzbench_architecture.png}}
\caption{FuzzBench main architecture visualized \cite{fuzzbench_docs}}
\label{fig:fuzzbench_architecture}
\end{figure}
\ \\
The process works as follows.
\newline \newline
Initially, a fuzzer developer integrates its product within FuzzBench using a Dockerfile, containing all the resources necessary to build targets using the fuzzer and where all benchmarks will be executed.
\newline
Similarly to OSS-Fuzz, this process is done via "pull requests", which are automatically revisioned and accepted by bots.
\newline \newline
Then, the developers may choose between two testing approaches: standard and OSS-Fuzz.
\newline
In the first case, the benchmark is created and defined by the developers themselves, and this requires the definition of fuzz targets, build files and Docker images that will be used to correctly build and link the fuzzer to the targets and run the tests.
\newline
In the latter, the developers employ a fuzz target from any OSS-Fuzz project as benchmark, allowing them to test their product on a real-world scenario.
\newline \newline
Finally, a report will be created highlighting the strengths and weaknesses of the fuzzer on the various benchmarks, comparing individual and overall results with other fuzzers.


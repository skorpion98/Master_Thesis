\chapter{Introduction}
\ \\

Nowadays, technology has permeated every aspect of human life, allowing everything and everyone to be interconnected anywhere and at any moment. Thanks to its importance, it quickly became one of the most popular fields of work in the modern age, with organizations employing and relying entirely on such technologies to provide their services. However, cybercrime has become very common. 

Cybersecurity is an aspect of informatics whose role is ensuring the safety of the technologies available and the people who use them, both in terms of software and infrastructure, protecting systems against security issues and vulnerabilities that could potentially be exploited by malicious entities. In fact, according to the ``Consortium for IT Software Quality'' (CISQ), poorly designed software cost the economy $\$2.08$ trillion dollars in 2020 alone \cite{forbes}, numbers that show how security against software bugs and vulnerabilities should be handled with maximum attention.

In this context, \textit{software security} aims to analyze and detect bugs in a program that may pose a security threat to any user and/or organization employing said software. The ``National Vulnerability Database'' (NVD) classified software vulnerabilities into 5 root causes \cite{nist}: input validation, access control, exception handling, configuration errors, and race condition. All these scenarios may lead to a criminal gaining access to your system and executing malicious code, potentially also with the intention of performing privilege escalation and taking ownership of the machine. 

One of the 7 components in the ``Software Development Life Cycle'' (SDLC) is \textit{Testing}, which means analyzing and checking your code to make sure that it satisfies some quality, correctness, and security criteria both in terms of functionalities provided and underlying infrastructure: this may either be done ``statically'', focusing on prevention and removal of potential defects from source code by means of coding standards and best practices, or ``dynamically'', analyzing the code during its execution with different inputs.

While testing a project is crucial to ensure its security and provide useful information that can be used to further improve the development of the product, it also involves fixing and solving many different kinds of problems affecting your code, which will obviously require time, resources, knowledge, and effort.   






\section{Context}
One of the most important goals of software testing is \textit{finding bugs}, defects in the code that cause unwanted and unexpected results, compromise the security of the product, and may lead to vulnerabilities being exploited by malicious people. In this context, a popular approach is \textit{automated testing}, where developers rely on external software to automatically perform repetitive tests and tasks over long periods of time without any human interaction. An example of an automated testing technique is \textit{fuzzing}, proposed in 1988 \cite{fuzz_proposed} and primarily used to automate simple tests previously performed by humans, which gained popularity due to its ability to discover crashes and bugs. Its base idea revolves around feeding (seemingly) random inputs to a program, analyzing its execution flow, and generating new testcases to explore all paths in the code as well as trigger unexpected behavior. This process is repeated across several and extensive fuzzing sessions, while its effectiveness and simplicity of use allow developers from all languages and levels of knowledge to improve the security of their software. It also discovered one of the most famous and critical bugs in history, the ``Heartbleed'' bug that affected OpenSSL.

Since 2001, with the rise of ``Open-Source Software'', ensuring the security of software that was freely available and modifiable by anyone became a top priority: because the source code was freely accessible, it was only a matter of time before cyber-criminals began exploiting their vulnerabilities. Moreover, many modern paid applications often rely on such free software to provide their services. 

Given their popularity, Google announced the ClusterFuzz project in 2012, a cloud-based fuzzing infrastructure for testing security-critical components of the Chromium web browser, where fuzzer developers could upload their fuzzing tool and be rewarded if their product detected a crash in the browser. In 2016, this infrastructure was repurposed as the back-end for OSS-Fuzz, a new campaign that provided continuous fuzzing at Google-scale for open-source developers. Later, in 2021, Google announced yet another new project called FuzzBench, which embraced the original purpose of ClusterFuzz and focused on supporting the development of open-source fuzzers thanks to tests based on real-world benchmarks and daily reports. All these frameworks use Google Cloud as their common sharing infrastructure, allowing them to easily share and access data independently. These automated fuzz-testing infrastructures are also valuable to researchers, who can use them to analyze and understand how open-source projects are developed and tested, access the same resources used by their developers during testing, and perform fuzzing locally for educational and research purposes.

While test automation is obviously a time-efficient solution compared to manual assessment, allowing machines to perfectly replicate the same repetitive task continuously and over extended periods of time with high consistency and accuracy, configuration and design choices may result in a partial loss of information. For this reason, the aforementioned automated fuzz-testing infrastructures might be missing bugs during their testing, and that is the main focus of this work.  




\section{Thesis Idea and Contributions}
This thesis focused on analyzing the workflow and results produced by automated testing infrastructures, more specifically OSS-Fuzz and FuzzBench: although these campaigns have enormous computing capabilities and have been performing continuous fuzzing for several years, they still suffer from machine errors and limitations due to their design choices. This work will show that these frameworks are in fact missing a relevant number of bugs, discovered by rebuilding a subset of the programs that are part of these campaigns and testing them locally using a set of novel tools and techniques for security analysis. All bugs found were then manually assessed, debugged (when possible), and finally reported to the respective developers hoping that this would help them make their software more secure.


\paragraph{Contributions.}
To summarize, this thesis makes the following contributions:
\begin{itemize}
    \item A study of the accuracy of existing continuous fuzzing frameworks.
    \item The detection of sources of overlooked bugs on state-of-the-art fuzzing frameworks.
    \item An assessment of the relevance of software sanitizers in fuzzing campaigns.
    \item The responsible disclosure of almost 250 bugs to the respective project maintainers.
\end{itemize}




\section{Outline}
This thesis is structured as follows. Chapter \ref{chap_2} provides all the necessary concepts to understand what is fuzzing and how a fuzzer works, along with notions about some tools that are widely used in this field. It also introduces the definition of ``Open-Source Program'' and the automated testing infrastructures analyzed in this work. Chapter \ref{chap_3} discusses the methodology applied during the selection of the projects and the test phase, including the problems faced during this process and the solutions found. Chapter \ref{chap_4} shows the results obtained, analyzing and talking about what could be inferred from them, and discusses the reports issued and the developers' responses. Chapter \ref{chap_5} ends this thesis by summarizing its contents, presenting some final considerations about fuzz testing, and introducing possible future works to expand this line of research.